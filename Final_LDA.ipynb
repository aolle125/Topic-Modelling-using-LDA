{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SQLContext\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, DoubleType, StringType, DateType, FloatType, ArrayType\n",
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "from pyspark.sql.functions import col, unix_timestamp, to_date\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import udf\n",
    "import re\n",
    "from pyspark.ml.clustering import LDA\n",
    "import string\n",
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer\n",
    "from pyspark.ml.feature import CountVectorizer, CountVectorizerModel, IDF\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master(\"local\").appName(\"Topic Modelling\").getOrCreate()\n",
    "#Create a spark context in session\n",
    "sc = spark.sparkContext\n",
    "#Reduce output by only showing me the errors\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "#SQL Context\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting a schema with Paper ID and Text\n",
    "schema = StructType([\n",
    "    StructField(\"Id\", StringType(), True),\n",
    "    StructField(\"Text\", StringType(), True)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the Dataframe with a Pre-defined Schema\n",
    "df = spark.read.csv(\"final_df.csv\", schema = schema, header = True, sep = \",\", multiLine=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make text lowercase, remove punctuations, remove text in square brackets and also remove and strip spaces\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\[.*?\\]', '', text)\n",
    "    text = re.sub('[\\W_]+', ' ', text)\n",
    "    text = re.sub(r'[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub(' +',' ',text.strip())\n",
    "    return text\n",
    "\n",
    "#Using User Defined Functions in Spark\n",
    "clean_udf = udf(lambda z: clean_text(z), StringType())\n",
    "df = df.withColumn(\"Text\", clean_udf('Text'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(['doi', 'preprint', 'copyright', 'peer',\\\n",
    "'reviewed', 'org', 'https', 'et', 'al', 'author', 'figure', \\\n",
    "    'rights', 'reserved', 'permission', 'used', 'using', 'biorxiv',\\\n",
    "'medrxiv', 'license', 'fig', 'al', 'Elsevier', 'PMC', 'CZI', 'www',\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\",\\\n",
    " \"ours\", \"ourselves\", \"you\", \"your\", \"yours\",\\\n",
    " \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \\\n",
    " \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \\\n",
    " \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \\\n",
    " \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \\\n",
    " \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\",\\\n",
    " \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\",\\\n",
    " \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \\\n",
    " \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\",\\\n",
    " \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\",\\\n",
    " \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\",\\\n",
    " \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"])\n",
    "\n",
    "stopwords = [clean_text(word) for word in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing Stop Words using UDF's and a manual set of stopwords\n",
    "def stop_word_removal(text):\n",
    "    text = ' '.join([word for word in text.split() if word not in stopwords and len(word) > 2])\n",
    "    return text\n",
    "\n",
    "stop_words_udf = udf(lambda z: stop_word_removal(z), StringType())\n",
    "df = df.withColumn(\"Text\", stop_words_udf('Text'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Limiting upto 100,000 entries out of ~220,000\n",
    "df = df.limit(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Sorting the count of all words in the documents in descending order \n",
    "#To make a graph having the top most 15 occuring terms in the corpus\n",
    "\n",
    "count_rdd = df.select('Text').rdd.flatMap(lambda x: x).flatMap(lambda x: x.split()).map(lambda word: (word, 1)).reduceByKey(lambda a, b: a + b)\n",
    "count_rdd = count_rdd.map(lambda x: (x[1],x[0])).sortByKey(ascending=False).map(lambda x: ((x[1],x[0])))\n",
    "top_15_words = count_rdd.take(15)\n",
    "words,count = map(list,zip(*top_15_words))\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.bar(words,count,color='navy')\n",
    "plt.xlabel(\"Words\") \n",
    "plt.ylabel(\"Count of the Words\") \n",
    "plt.title(\"Top 15 most occuring words\") \n",
    "plt.show() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Distribution of word counts across all documents\n",
    "\n",
    "def doc_word_counts(text):\n",
    "    text = text.split()\n",
    "    return len(text)\n",
    "doc_len_udf = udf(lambda z: doc_word_counts(z), IntegerType())\n",
    "df = df.withColumn(\"DocLen\", doc_len_udf('Text'))\n",
    "answer = df.select(\"DocLen\").collect()\n",
    "lengths = [row.DocLen for row in answer]\n",
    "lengths = sorted(lengths,reverse=True)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.hist(lengths, bins = 1000, color='navy')\n",
    "plt.text(750, 700, \"Mean   : \" + str(round(np.mean(lengths))))\n",
    "plt.text(750, 650, \"Median : \" + str(round(np.median(lengths))))\n",
    "plt.text(750, 600, \"Stdev   : \" + str(round(np.std(lengths))))\n",
    "plt.text(750, 550, \"1%ile    : \" + str(round(np.quantile(lengths, q=0.01))))\n",
    "plt.text(750, 500, \"99%ile  : \" + str(round(np.quantile(lengths, q=0.99))))\n",
    "\n",
    "plt.gca().set(xlim=(0, 1000), xlabel='Document Word Count', ylabel='Number of Documents')\n",
    "plt.tick_params(size=16)\n",
    "plt.xticks(np.linspace(0,1000,9))\n",
    "plt.title('Distribution of Document Word Counts', fontdict=dict(size=22))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a Pipeline which has a Tokenizer, a Count Vectorizer and an IDF function\n",
    "tokenizer = Tokenizer(inputCol=\"Text\", outputCol=\"Text_Tokens\")\n",
    "cv = CountVectorizer(inputCol=\"Text_Tokens\", outputCol=\"Vectors\", maxDF = 0.9, minTF = 2, minDF = 0.005, vocabSize=10000)\n",
    "idf = IDF(inputCol=\"Vectors\", outputCol=\"IDFVectors\")\n",
    "\n",
    "#Setting up the Pipeline with the functions\n",
    "pipeline = Pipeline(stages=[tokenizer,cv, idf])\n",
    "model = pipeline.fit(df)\n",
    "df_final = model.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding Perplexity and LogLikelihood for all values of k, for finding the optimum value of number of topics\n",
    "\n",
    "k_trial = [10,25,30,35,40,50,75]\n",
    "ll_final = []\n",
    "lp_final = []\n",
    "for k in k_trial:\n",
    "    lda = LDA(featuresCol = 'IDFVectors', k = k)\n",
    "    model_lda = lda.fit(df_final)\n",
    "    ll = model_lda.logLikelihood(df_final)\n",
    "    lp = model_lda.logPerplexity(df_final)\n",
    "    print(\"The lower bound on the log likelihood of the entire corpus: \" + str(ll))\n",
    "    print(\"The upper bound on perplexity: \" + str(lp))\n",
    "    ll_final.append(ll)\n",
    "    lp_final.append(lp)\n",
    "    \n",
    "fig, axes = plt.subplots(1, 2, figsize=(10,10))\n",
    "axes[0].plot(ll_final)\n",
    "axes[1].plot(lp_final) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting LDA for the best value of k\n",
    "lda = LDA(featuresCol = 'IDFVectors', k = 25)\n",
    "model_lda = lda.fit(df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Collecting all the topics\n",
    "topics = model_lda.describeTopics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to find the term in every topic from the vocabulary\n",
    "def termsIdx2Term(termIndices):\n",
    "    return [vocab[int(index)] for index in termIndices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Collecting the vocabulary and all the topics with the terms\n",
    "vectorizer = model.stages[1]     \n",
    "vocab = vectorizer.vocabulary\n",
    "termsIdx2Term = udf(termsIdx2Term, ArrayType(StringType()))\n",
    "topics = topics.withColumn('Terms',termsIdx2Term('termIndices'))\n",
    "terms = topics.select('Terms').collect()\n",
    "terms = [row.Terms for row in terms]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the wordcloud for every topic\n",
    "\n",
    "fig, axes = plt.subplots(12, 2, figsize=(30,30))\n",
    "\n",
    "\n",
    "cloud = WordCloud(background_color='white',\n",
    "                  width=2500,\n",
    "                  height=1800,\n",
    "                  prefer_horizontal=1.0)\n",
    "\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    fig.add_subplot(ax)\n",
    "    text = ' '.join(terms[i])\n",
    "    wordcloud = cloud.generate(text)\n",
    "    plt.gca().imshow(wordcloud)\n",
    "    plt.gca().set_title('Topic ' + str(i), fontdict=dict(size=16))\n",
    "    plt.gca().axis('off')\n",
    "    \n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "plt.axis('off')\n",
    "plt.margins(x=0, y=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Collecting the Word Count for every term in the set of topics\n",
    "word_counts = count_rdd.collect()\n",
    "final_counts = []\n",
    "for i in range(len(terms)):\n",
    "    final_counts.append([item for item in word_counts if item[0] in terms[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding the topic for every document in the given DataFrame which is the topic which has highest distribution in the list\n",
    "getMainTopic = udf(lambda l: int(np.argmax([float(x) for x in l])), IntegerType())\n",
    " \n",
    "countTopDocs = (model_lda.transform(df_final).select(getMainTopic(\"topicDistribution\").alias(\"idxMainTopic\"))\n",
    "                .groupBy(\"idxMainTopic\").count().sort(\"idxMainTopic\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the number of documents for every topic\n",
    "\n",
    "countTopDocs = countTopDocs.withColumnRenamed('count','counts')\n",
    "count_topics = countTopDocs.select('counts').collect()\n",
    "count_topics = [row.counts for row in count_topics]\n",
    "topics = countTopDocs.select('idxMainTopic').collect()\n",
    "topics = [row.idxMainTopic for row in topics]\n",
    "\n",
    "#Sorting the two lists to get count from maximum to minimum count of topic vs document counts\n",
    "sorting = [(a,b) for a,b in zip(topics,count_topics)]\n",
    "sorting = sorted(sorting,key=lambda x: x[1], reverse=True)\n",
    "topics = list(zip(*sorting))[0]\n",
    "count_topics = list(zip(*sorting))[1]\n",
    "\n",
    "plt.figure(figsize = (10,10))\n",
    "plt.bar(range(len(count_topics)),count_topics,color='navy', align = 'center')\n",
    "plt.xlabel(\"Topics\") \n",
    "plt.ylabel(\"Count of the Topics\") \n",
    "plt.title(\"Number of Documents every topic\") \n",
    "plt.xticks(ticks=np.arange(0,24,1), labels = topics)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the word counts for every term throughout every topic in the corpus\n",
    "\n",
    "fig, axes = plt.subplots(12, 2, figsize=(70,70), sharex=False, sharey=False)\n",
    "plt.rcParams.update({'font.size': 38})\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    words = [item[0] for item in final_counts[i]]\n",
    "    counts = [item[1] for item in final_counts[i]]\n",
    "    fig.add_subplot(ax)\n",
    "    ax.bar(words,counts)\n",
    "    plt.gca().set_title('Topic ' + str(i), fontdict=dict(size=32))\n",
    "\n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "plt.margins(x=0, y=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
